{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 â€“ Feature Engineering\n",
        "\n",
        "Blend Fitbit wearables, Chengdu PM2.5 air quality, and Delhi weather data into a unified feature table ready for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: fitbit=dailyActivity_merged.csv (940 rows), air=ChengduPM20100101_20151231.csv (52584 rows), weather=DailyDelhiClimateTrain.csv (1462 rows)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from federated_health_risk.utils.config import init_env\n",
        "\n",
        "# initialize env (loads .env if present)\n",
        "init_env()\n",
        "\n",
        "# repository-local data paths (Windows-safe)\n",
        "repo_root = Path(r\"C:\\Users\\Sahal Saeed\\Documents\\7semester\\mlops\\project_cursor\")\n",
        "data_dir = repo_root / \"data\"\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "features_dir = data_dir / \"features\"\n",
        "features_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# allow overrides via environment variables set by init_env()\n",
        "fitbit_path = Path(os.getenv(\"FITBIT_CSV\", data_dir / \"fitbit\" / \"dailyActivity_merged.csv\"))\n",
        "air_path = Path(os.getenv(\"AIR_QUALITY_CSV\", data_dir / \"air_quality\" / \"ChengduPM20100101_20151231.csv\"))\n",
        "weather_path = Path(os.getenv(\"WEATHER_CSV\", data_dir / \"weather\" / \"DailyDelhiClimateTrain.csv\"))\n",
        "\n",
        "# fail fast if inputs are missing (clear error message)\n",
        "missing = [p for p in (fitbit_path, air_path, weather_path) if not p.exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(f\"Missing input files: {missing}\")\n",
        "\n",
        "# load inputs\n",
        "fitbit_df = pd.read_csv(fitbit_path)\n",
        "air_df = pd.read_csv(air_path)\n",
        "weather_df = pd.read_csv(weather_path)\n",
        "\n",
        "print(f\"Loaded: fitbit={fitbit_path.name} ({len(fitbit_df)} rows), air={air_path.name} ({len(air_df)} rows), weather={weather_path.name} ({len(weather_df)} rows)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitbit_df[\"ActivityDate\"] = pd.to_datetime(fitbit_df[\"ActivityDate\"], errors=\"coerce\", format=\"%m/%d/%Y\")\n",
        "fitbit_df = fitbit_df.dropna(subset=[\"ActivityDate\"])\n",
        "fitbit_df[\"date\"] = fitbit_df[\"ActivityDate\"].dt.floor(\"D\")\n",
        "fitbit_daily = (\n",
        "    fitbit_df.groupby(\"date\")\n",
        "    .agg(\n",
        "        total_steps=(\"TotalSteps\", \"mean\"),\n",
        "        active_minutes=(\"VeryActiveMinutes\", \"mean\"),\n",
        "        sedentary_minutes=(\"SedentaryMinutes\", \"mean\"),\n",
        "        calories=(\"Calories\", \"mean\"),\n",
        "        distance_km=(\"TotalDistance\", \"mean\"),\n",
        "    )\n",
        "    .reset_index()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "air_daily rows: 2191\n"
          ]
        }
      ],
      "source": [
        "# --- Air quality: clean columns and build date robustly ---\n",
        "# Trim column names and replace \"NA\" strings\n",
        "air_df.columns = air_df.columns.str.strip()\n",
        "air_df = air_df.replace({\"NA\": np.nan})\n",
        "\n",
        "# Ensure PM columns exist and coerce to numeric\n",
        "pm_cols = [c for c in [\"PM_Caotangsi\", \"PM_Shahepu\", \"PM_US Post\"] if c in air_df.columns]\n",
        "if pm_cols:\n",
        "    air_df[pm_cols] = air_df[pm_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Normalize year/month/day column names (case-insensitive)\n",
        "lower_map = {c.lower(): c for c in air_df.columns}\n",
        "year_col = lower_map.get(\"year\")\n",
        "month_col = lower_map.get(\"month\")\n",
        "day_col = lower_map.get(\"day\")\n",
        "\n",
        "# Assemble date from year/month/day if present, else try any date-like column\n",
        "if year_col and month_col and day_col:\n",
        "    air_df[[year_col, month_col, day_col]] = air_df[[year_col, month_col, day_col]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    air_df[\"date\"] = pd.to_datetime(dict(year=air_df[year_col].astype(\"Int64\"), month=air_df[month_col].astype(\"Int64\"), day=air_df[day_col].astype(\"Int64\")), errors=\"coerce\")\n",
        "else:\n",
        "    possible_date_cols = [c for c in air_df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\n",
        "    if possible_date_cols:\n",
        "        air_df[\"date\"] = pd.to_datetime(air_df[possible_date_cols[0]], errors=\"coerce\")\n",
        "    else:\n",
        "        raise ValueError(\"Air dataframe does not contain year/month/day or a date-like column.\")\n",
        "\n",
        "air_df = air_df.dropna(subset=[\"date\"])\n",
        "air_df[\"date\"] = air_df[\"date\"].dt.floor(\"D\")\n",
        "\n",
        "# Aggregate daily (keep only numeric aggregations)\n",
        "agg_map = {}\n",
        "if \"PM_US Post\" in air_df.columns:\n",
        "    agg_map[\"pm_us_post\"] = (\"PM_US Post\", \"mean\")\n",
        "elif pm_cols:\n",
        "    agg_map[\"pm_us_post\"] = (pm_cols[0], \"mean\")\n",
        "if \"PM_Caotangsi\" in air_df.columns:\n",
        "    agg_map[\"pm_caotangsi\"] = (\"PM_Caotangsi\", \"mean\")\n",
        "if \"PM_Shahepu\" in air_df.columns:\n",
        "    agg_map[\"pm_shahepu\"] = (\"PM_Shahepu\", \"mean\")\n",
        "# optional meteorological cols if present\n",
        "for col in [\"DEWP\", \"HUMI\", \"PRES\", \"TEMP\"]:\n",
        "    if col in air_df.columns:\n",
        "        agg_map[col.lower()] = (col, \"mean\")\n",
        "\n",
        "if not agg_map:\n",
        "    raise ValueError(\"No aggregatable columns found in air_df after parsing.\")\n",
        "\n",
        "air_daily = air_df.groupby(\"date\").agg(**agg_map).reset_index()\n",
        "\n",
        "print(\"air_daily rows:\", len(air_daily))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved per-node feature files to: C:\\Users\\Sahal Saeed\\Documents\\7semester\\mlops\\project_cursor\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "# --- Build per-node feature tables (NO central merge) ---\n",
        "processed_dir = data_dir / \"processed\"\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------- Fitbit features ----------------\n",
        "fitbit_daily[\"cardio_load\"] = (\n",
        "    fitbit_daily[\"total_steps\"] /\n",
        "    (fitbit_daily[\"active_minutes\"].replace(0, np.nan) + 1)\n",
        ")\n",
        "\n",
        "fitbit_daily[\"steps_per_km\"] = (\n",
        "    fitbit_daily[\"total_steps\"] /\n",
        "    fitbit_daily[\"distance_km\"].replace(0, np.nan)\n",
        ")\n",
        "\n",
        "fitbit_daily = fitbit_daily.replace([np.inf, -np.inf], np.nan)\n",
        "fitbit_daily.to_csv(processed_dir / \"fitbit_daily_features.csv\", index=False)\n",
        "fitbit_daily.to_parquet(features_dir / \"fitbit_daily_features.parquet\", index=False)\n",
        "\n",
        "# ---------------- Air quality features ----------------\n",
        "available_pm = [c for c in [\"pm_us_post\", \"pm_caotangsi\", \"pm_shahepu\"] if c in air_daily.columns]\n",
        "\n",
        "air_daily[\"pm_mean\"] = (\n",
        "    air_daily[available_pm].mean(axis=1, skipna=True)\n",
        "    if available_pm else np.nan\n",
        ")\n",
        "\n",
        "air_daily[\"pm_log\"] = np.log1p(air_daily[\"pm_mean\"].clip(lower=0))\n",
        "air_daily[\"pm_exceed_100\"] = (air_daily[\"pm_mean\"] > 100).astype(int)\n",
        "\n",
        "air_daily = air_daily.replace([np.inf, -np.inf], np.nan)\n",
        "air_daily.to_csv(processed_dir / \"air_daily_features.csv\", index=False)\n",
        "air_daily.to_parquet(features_dir / \"air_daily_features.parquet\", index=False)\n",
        "\n",
        "\n",
        "# ---------------- Weather features ----------------\n",
        "if \"delhi_meantemp\" not in weather_daily.columns and \"meantemp\" in weather_daily.columns:\n",
        "    weather_daily = weather_daily.rename(columns={\"meantemp\": \"delhi_meantemp\"})\n",
        "\n",
        "weather_daily[\"heat_index_proxy\"] = (\n",
        "    weather_daily.get(\"delhi_meantemp\", 0) +\n",
        "    0.1 * weather_daily.get(\"delhi_humidity\", 0)\n",
        ")\n",
        "\n",
        "weather_daily = weather_daily.replace([np.inf, -np.inf], np.nan)\n",
        "weather_daily.to_csv(processed_dir / \"weather_daily_features.csv\", index=False)\n",
        "weather_daily.to_parquet(features_dir / \"weather_daily_features.parquet\", index=False)\n",
        "\n",
        "print(f\"Saved per-node feature files to: {processed_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weather_daily rows: 1462\n"
          ]
        }
      ],
      "source": [
        "# --- Build weather_daily from weather_df (robust) ---\n",
        "# try common date column names first\n",
        "weather_df['date'] = pd.to_datetime(weather_df.get('date', weather_df.get('Date', None)), errors='coerce')\n",
        "# if that fails, try assembling from year/month/day\n",
        "if weather_df['date'].isna().all():\n",
        "    for col in ['year','Year']:\n",
        "        if col in weather_df.columns:\n",
        "            weather_df['year'] = pd.to_numeric(weather_df[col], errors='coerce')\n",
        "    for col in ['month','Month']:\n",
        "        if col in weather_df.columns:\n",
        "            weather_df['month'] = pd.to_numeric(weather_df[col], errors='coerce')\n",
        "    for col in ['day','Day']:\n",
        "        if col in weather_df.columns:\n",
        "            weather_df['day'] = pd.to_numeric(weather_df[col], errors='coerce')\n",
        "    if {'year','month','day'}.issubset(weather_df.columns):\n",
        "        weather_df['date'] = pd.to_datetime(dict(year=weather_df['year'], month=weather_df['month'], day=weather_df['day']), errors='coerce')\n",
        "\n",
        "# drop rows without a parsable date\n",
        "weather_df = weather_df.dropna(subset=['date'])\n",
        "weather_df['date'] = weather_df['date'].dt.floor('D')\n",
        "\n",
        "# normalize common column names to those used downstream\n",
        "lower_map = {c.lower(): c for c in weather_df.columns}\n",
        "rename_map = {}\n",
        "if 'meantemp' in lower_map:\n",
        "    rename_map[lower_map['meantemp']] = 'delhi_meantemp'\n",
        "if 'humidity' in lower_map:\n",
        "    rename_map[lower_map['humidity']] = 'delhi_humidity'\n",
        "if 'wind_speed' in lower_map:\n",
        "    rename_map[lower_map['wind_speed']] = 'delhi_wind_speed'\n",
        "if 'meanpressure' in lower_map:\n",
        "    rename_map[lower_map['meanpressure']] = 'delhi_meanpressure'\n",
        "if rename_map:\n",
        "    weather_df = weather_df.rename(columns=rename_map)\n",
        "\n",
        "# aggregate daily numeric values\n",
        "weather_daily = weather_df.groupby('date').mean(numeric_only=True).reset_index()\n",
        "\n",
        "print('weather_daily rows:', len(weather_daily))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# features = (\n",
        "#     fitbit_daily.merge(air_daily, on=\"date\", how=\"inner\")\n",
        "#     .merge(weather_daily, on=\"date\", how=\"inner\")\n",
        "#     .dropna()\n",
        "# )\n",
        "# features[\"cardio_load\"] = features[\"total_steps\"] / (features[\"active_minutes\"] + 1)\n",
        "# features[\"pollution_load\"] = features[\"pm_mean\"] * features[\"delhi_meantemp\"]\n",
        "# features[\"risk_proxy\"] = (\n",
        "#     0.4 * features[\"pm_mean\"]\n",
        "#     + 0.2 * features[\"delhi_humidity\"]\n",
        "#     + 0.2 * features[\"delhi_meantemp\"]\n",
        "#     + 0.2 * features[\"cardio_load\"]\n",
        "# ) / 100\n",
        "\n",
        "# features.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# output_path = features_dir / \"multimodal_features.parquet\"\n",
        "# features.to_parquet(output_path, index=False)\n",
        "# print(f\"Saved {len(features)} rows to {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
